{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtDsH_OnG-FC"
      },
      "source": [
        "# Logistic Regression Gradient & Convexity — Student Lab\n",
        "\n",
        "Complete all TODOs. This lab is math-first and stability-first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hz755MdtG-FE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwg4FE7EG-FF"
      },
      "source": [
        "## Section 0 — Synthetic Dataset\n",
        "We’ll create a binary classification dataset with both separable and non-separable regimes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucTImZjTG-FF",
        "outputId": "5f2ff300-72b4-4833-f86c-df175f1b0f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: shapes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((400, 5), np.float64(0.515))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "def make_data(n=400, d=5, separable=False):\n",
        "    X = rng.standard_normal((n, d))\n",
        "    w_true = rng.standard_normal(d)\n",
        "    logits = X @ w_true\n",
        "    if separable:\n",
        "        y = (logits > 0).astype(int)\n",
        "    else:\n",
        "        # add noise so it's not perfectly separable\n",
        "        logits = logits + 0.5 * rng.standard_normal(n)\n",
        "        probs = 1 / (1 + np.exp(-logits))\n",
        "        y = (rng.random(n) < probs).astype(int)\n",
        "    return X, y, w_true\n",
        "\n",
        "X, y, w_true = make_data(separable=False)\n",
        "check('shapes', X.ndim==2 and y.ndim==1 and X.shape[0]==y.shape[0])\n",
        "X.shape, y.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSVpABO-G-FF"
      },
      "source": [
        "## Section 1 — Sigmoid + Stable Loss\n",
        "\n",
        "### Task 1.1: Stable sigmoid\n",
        "\n",
        "# TODO: implement a numerically stable sigmoid.\n",
        "# HINT:\n",
        "# - Use `np.where(z>=0, ...)` trick to avoid overflow.\n",
        "\n",
        "**Explain:** Why does sigmoid saturate for large |z| and what does that do to gradients?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnWtDsEvG-FG",
        "outputId": "647174e1-3cf8-4a0b-9ba7-7da53ecc6aef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00000000e+00 4.53978687e-05 5.00000000e-01 9.99954602e-01\n",
            " 1.00000000e+00]\n",
            "OK: in_0_1\n",
            "OK: monotonic\n"
          ]
        }
      ],
      "source": [
        "def sigmoid(z):\n",
        "    #return np.where(z>=0, 1/(1+np.exp(-z)), np.exp(z)/(1+np.exp(z)))\n",
        "    positive_pos = z >= 0\n",
        "    negative_pos = z< 0\n",
        "\n",
        "    result = np.zeros_like(z)\n",
        "    result[positive_pos] = 1 / (1+ np.exp(-z[positive_pos]))\n",
        "    result[negative_pos] = np.exp(z[negative_pos]) / (1+ np.exp(z[negative_pos]))\n",
        "    return result\n",
        "\n",
        "z = np.array([-1000.0, -10.0, 0.0, 10.0, 1000.0])\n",
        "p = sigmoid(z)\n",
        "print(p)\n",
        "check('in_0_1', np.all((p >= 0) & (p <= 1)))\n",
        "check('monotonic', np.all(np.diff(p) >= 0))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcoQZeF0G-FG"
      },
      "source": [
        "### Task 1.2: Stable binary cross-entropy loss\n",
        "\n",
        "We use labels y in {0,1}.\n",
        "\n",
        "Loss per example: `-y log(p) - (1-y) log(1-p)` where p=sigmoid(z).\n",
        "\n",
        "# TODO: implement stable loss without NaNs/inf.\n",
        "# HINT:\n",
        "# - Clip p to [eps, 1-eps]\n",
        "# - Alternatively use softplus: log(1+exp(z))\n",
        "\n",
        "**Interview Angle:** explain a stable form of log-loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SPkNpjWG-FG",
        "outputId": "52871ce2-2c2b-4fd3-adc1-89842d086c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.6931471805599452\n",
            "OK: finite_loss\n"
          ]
        }
      ],
      "source": [
        "def bce_loss(y, p):\n",
        "    # TODO\n",
        "    eps = 1e-12\n",
        "    p = np.clip(p, eps,1-eps)\n",
        "    loss = -(y * np.log(p) + (1-y) * np.log(1-p))\n",
        "    return np.mean(loss)\n",
        "\n",
        "# sanity: loss is finite\n",
        "z = X @ np.zeros(X.shape[1])\n",
        "p = sigmoid(z)\n",
        "L = bce_loss(y, p)\n",
        "print('loss', L)\n",
        "check('finite_loss', np.isfinite(L))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRAWnqgvG-FG"
      },
      "source": [
        "## Section 2 — Gradient (Derive → Implement → Check)\n",
        "\n",
        "### Task 2.1: Derive the gradient (write in markdown)\n",
        "Show that for loss averaged over n examples:\n",
        "`grad = X^T (p - y) / n`\n",
        "\n",
        "**Checkpoint:** What is the shape of grad?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loss = -(y * log(p) + (1-y)*log(1-p))\n",
        "\n",
        "# d(p)/dw = X.T * p (1-p)\n",
        "\n",
        "# Now differentiating this with respect to weights:\n",
        "\n",
        "# d (loss)/ d(w) = - (y * (1/p) * d (p)/dw - (1-y)d(p)/dw/(1-p))\n",
        "\n",
        "#                = - ( d (p)/dw( y/p - (1-y)/(1-p)))\n",
        "\n",
        "#                = - ( X.t * p (1-p)( y/p - (1-y)/(1-p)))\n",
        "\n",
        "#                = - ( X.t * p (1-p)( (y- yp + yp - p)/p(1-p)))\n",
        "\n",
        "#                = - ( X.t * p (1-p)( (y- p)/p(1-p)))\n",
        "\n",
        "#                = - ( X.t @ (y- p))\n",
        "\n",
        "\n",
        "# Shape of grad is the same shape as the weights"
      ],
      "metadata": {
        "id": "PsVI52IZt-6Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlR_LU6PG-FH"
      },
      "source": [
        "### Task 2.2: Implement loss and gradient\n",
        "\n",
        "# TODO: implement `logreg_loss_and_grad(X, y, w)` returning (loss, grad).\n",
        "# HINT:\n",
        "# - z = X @ w\n",
        "# - p = sigmoid(z)\n",
        "# - loss = BCE\n",
        "# - grad = X.T @ (p - y) / n\n",
        "\n",
        "**FAANG gotcha:** ensure y is 0/1, not -1/+1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-wIQL9IG-FI",
        "outputId": "d8e12a8f-d608-47df-b657-37d553798688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 1.1070610960831098 grad_norm 0.4713027205170631\n",
            "OK: grad_shape\n",
            "OK: finite\n"
          ]
        }
      ],
      "source": [
        "def logreg_loss_and_grad(X, y, w):\n",
        "    # TODO\n",
        "    pred = sigmoid( X @ w)\n",
        "    loss = bce_loss(y,pred)\n",
        "    grad = (X.T @ (pred-y) )/ X.shape[0]\n",
        "    assert grad.shape[0] == w.shape[0], \"Grad and Weight Shape do not match\"\n",
        "    return loss, grad\n",
        "\n",
        "w0 = rng.standard_normal(X.shape[1])\n",
        "loss, grad = logreg_loss_and_grad(X, y, w0)\n",
        "print('loss', loss, 'grad_norm', np.linalg.norm(grad))\n",
        "check('grad_shape', grad.shape == w0.shape)\n",
        "check('finite', np.isfinite(loss) and np.all(np.isfinite(grad)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV8eBOMhG-FI"
      },
      "source": [
        "### Task 2.3: Numerical gradient check (finite differences)\n",
        "\n",
        "# TODO: implement numerical gradient and compare to analytic grad.\n",
        "# HINT: central difference\n",
        "\n",
        "**Checkpoint:** Why can eps too small make the check worse?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67_rcbPBG-FI",
        "outputId": "1f4edb00-a3ff-4dec-b61c-ddb400c8b745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_abs_diff 1.7092688375797138e-11\n",
            "OK: grad_check\n"
          ]
        }
      ],
      "source": [
        "def numerical_grad(f, w, eps=1e-5):\n",
        "    g = np.zeros_like(w)\n",
        "    for i in range(w.shape[0]):\n",
        "      e = np.zeros_like(w)\n",
        "      e[i] = 1.0\n",
        "      g[i] = (f( w + eps * e) - f( w - eps * e )) / (2*eps)\n",
        "    return g\n",
        "\n",
        "f = lambda v: logreg_loss_and_grad(X, y, v)[0]\n",
        "g_num = numerical_grad(f, w0)\n",
        "_, g = logreg_loss_and_grad(X, y, w0)\n",
        "\n",
        "max_abs = np.max(np.abs(g - g_num))\n",
        "print('max_abs_diff', max_abs)\n",
        "check('grad_check', max_abs < 1e-5)\n",
        "\n",
        "# When eps is small, the change can become very small, and the numerical gradient might get very small or 0 due to floating point precision limits being hit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xveFrW6IG-FI"
      },
      "source": [
        "## Section 3 — Convexity & Hessian Intuition\n",
        "\n",
        "### Task 3.1: Hessian-vector product (HVP)\n",
        "\n",
        "For logistic regression:\n",
        "H = (1/n) X^T S X where S = diag(p(1-p)).\n",
        "\n",
        "Implement HVP: compute H@v without building full H explicitly.\n",
        "\n",
        "# HINT:\n",
        "- s = p*(1-p)\n",
        "- compute Xv then multiply by s then X^T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zr8FsmvG-FI",
        "outputId": "a88111fa-965a-4ef5-a304-d73c7f816d40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: hvp_shape\n",
            "OK: hvp_finite\n"
          ]
        }
      ],
      "source": [
        "def hvp(X, y, w, v):\n",
        "    # TODO\n",
        "    p = X @ w\n",
        "    z = sigmoid(p)\n",
        "    s = z * (1-z)\n",
        "    Xv = X @ v\n",
        "    return (X.T @ (s * Xv))/X.shape[0]\n",
        "\n",
        "v = rng.standard_normal(X.shape[1])\n",
        "Hv = hvp(X, y, w0, v)\n",
        "check('hvp_shape', Hv.shape == v.shape)\n",
        "check('hvp_finite', np.all(np.isfinite(Hv)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHWjGPG8G-FI"
      },
      "source": [
        "### Task 3.2: Empirical PSD check\n",
        "\n",
        "Check that v^T H v >= 0 for random v (PSD).\n",
        "\n",
        "**Explain:** Why does PSD imply convexity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "tHVdDBiAG-FI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c9c93a4-0cb5-4399-b118-886c97989a2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min vHv 0.11845886678983329\n",
            "OK: psd\n"
          ]
        }
      ],
      "source": [
        "def vHv(X, y, w, v):\n",
        "    return float(v @ hvp(X, y, w, v))\n",
        "\n",
        "vals = []\n",
        "for _ in range(50):\n",
        "    v = rng.standard_normal(X.shape[1])\n",
        "    vals.append(vHv(X, y, w0, v))\n",
        "print('min vHv', min(vals))\n",
        "check('psd', min(vals) > -1e-8)\n",
        "\n",
        "# PSD means curvature is non-negative in all directions. This means there are no dips/valleys anywhere implying we are in a bowl shape and hence convex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33vgab9pG-FI"
      },
      "source": [
        "## Section 4 — Optimization (Bonus)\n",
        "\n",
        "### Task 4.1: One step of GD vs Newton (conceptual)\n",
        "\n",
        "Implement one gradient descent step. (Newton step is optional.)\n",
        "\n",
        "**FAANG gotcha:** perfectly separable data can push weights to infinity; explain why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "wb2cv77_G-FI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b7b1ab-997c-4f19-f945-0b45dc375ad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss0 1.1070610960831098 loss1 1.0849868544909549\n",
            "OK: decreased\n"
          ]
        }
      ],
      "source": [
        "def gd_step(X, y, w, lr):\n",
        "    _, grad = logreg_loss_and_grad(X,y,w)\n",
        "    return  w - lr * grad\n",
        "\n",
        "w1 = gd_step(X, y, w0, lr=0.1)\n",
        "loss0, _ = logreg_loss_and_grad(X, y, w0)\n",
        "loss1, _ = logreg_loss_and_grad(X, y, w1)\n",
        "print('loss0', loss0, 'loss1', loss1)\n",
        "check('decreased', loss1 <= loss0 + 1e-12)\n",
        "\n",
        "#Gradient descent will keep pushing the weights higher to make the predictions closer to 0 and 1 but since it is sigmoid function, it only approahces 1 but not exactly.\n",
        "# So loss keeps reducing as it keeps getting closer and closer to predicting 1, but it will never be 1 and hence weights keep increasing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sqJOxHqG-FJ"
      },
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Gradient check passes\n",
        "- PSD check passes\n",
        "- Short answers to checkpoint questions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JUYEwPpgV2qa"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}